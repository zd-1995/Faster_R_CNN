{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8042445,"sourceType":"datasetVersion","datasetId":4741736},{"sourceId":8042471,"sourceType":"datasetVersion","datasetId":4741755},{"sourceId":8042921,"sourceType":"datasetVersion","datasetId":4742098},{"sourceId":8043091,"sourceType":"datasetVersion","datasetId":4742225},{"sourceId":8043291,"sourceType":"datasetVersion","datasetId":4742369},{"sourceId":8043392,"sourceType":"datasetVersion","datasetId":4742443},{"sourceId":8043543,"sourceType":"datasetVersion","datasetId":4742546},{"sourceId":8044027,"sourceType":"datasetVersion","datasetId":4742907},{"sourceId":8072358,"sourceType":"datasetVersion","datasetId":4763290},{"sourceId":8107457,"sourceType":"datasetVersion","datasetId":4788730}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\n# dataset load\n!pip install roboflow\n\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"8CzHJJr0mbSZN5Yndqlp\")\nproject = rf.workspace(\"project-986i8\").project(\"drone-uskpc\")\ndataset = project.version(1).download(\"tensorflow\")\n\n'''","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":70.064001,"end_time":"2024-03-01T22:38:48.933196","exception":false,"start_time":"2024-03-01T22:37:38.869195","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-25T15:33:20.335293Z","iopub.execute_input":"2024-03-25T15:33:20.335642Z","iopub.status.idle":"2024-03-25T15:35:03.137800Z","shell.execute_reply.started":"2024-03-25T15:33:20.335613Z","shell.execute_reply":"2024-03-25T15:35:03.136569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# dataset load\n!pip install roboflow\n\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"8CzHJJr0mbSZN5Yndqlp\")\nproject = rf.workspace(\"project-986i8\").project(\"drone-uskpc\")\ndataset = project.version(1).download(\"yolov8\")\n\n'''","metadata":{"execution":{"iopub.status.busy":"2024-04-02T12:25:00.330575Z","iopub.execute_input":"2024-04-02T12:25:00.331000Z","iopub.status.idle":"2024-04-02T12:26:24.887426Z","shell.execute_reply.started":"2024-04-02T12:25:00.330970Z","shell.execute_reply":"2024-04-02T12:26:24.886513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### all data collection","metadata":{}},{"cell_type":"code","source":"#-- Create directory structure as follows:\n'''\ndone_ds:\n    train:\n        images\n        labels\n\n    valid:\n        images\n        labels\n\n    test:\n        images\n        labels\n'''\nimport os\nimport shutil\n\n\n#-- Create Empty folders -------------------------------------------------------------\n\n#-- Define the path for the main folder --\nmain_folder = 'drone_ds'\n\n#-- Define subfolders --\nsubfolders = ['train', 'valid', 'test']\nsubsubfolders = ['images', 'labels']\n\n#-- save created directories as a dict --\ndir_dict = {}\n\n#-- Create the main folder --\nos.makedirs(main_folder, exist_ok=True)\n\n#-- Create subfolders and sub-subfolders --\nfor folder in subfolders:\n    os.makedirs(os.path.join(main_folder, folder), exist_ok=True)\n   \n    for subfolder in subsubfolders:\n        new_dir = os.path.join(main_folder, folder, subfolder)\n        os.makedirs(new_dir, exist_ok=True)\n        dir_dict[(folder,subfolder)] = new_dir\n\n\n# print(dir_dict)\n#--------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-04-19T12:08:28.865904Z","iopub.execute_input":"2024-04-19T12:08:28.866501Z","iopub.status.idle":"2024-04-19T12:08:28.874630Z","shell.execute_reply.started":"2024-04-19T12:08:28.866468Z","shell.execute_reply":"2024-04-19T12:08:28.873602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#-- fild paths for images and labeles --------------------------------------------------\ndef find_paths(directory, extensions):\n    \n    file_paths = set() \n    \n    #-- Walk through the directory tree --\n    for root, _, files in os.walk(directory):        \n        for file in files:            \n            if any(file.endswith(ext) for ext in extensions):\n                file_paths.add(root)                \n                break\n    \n    return file_paths\n#--------------------------------------------------------------------------------------\n\n#-- find all paths for images and labels ----------------------------------------------\n\n#-- Define the directory to search in --\ndirectory_to_search = '/kaggle/input'\n\n#-- Define the file extensions you want to search for --\nimage_extensions = {'.jpg', '.jpeg', '.png', '.JPEG'}  \ntext_extensions = {'.txt'}  \n\n#-- Find paths to image files --\nimages_paths = find_paths(directory_to_search, image_extensions)\n\n#-- Find paths to label files --\nlabels_paths = find_paths(directory_to_search, text_extensions)\n\n#-- Print the paths --\nprint(\"Image paths:\")\nfor img_path in images_paths:\n    print(img_path)\n\nprint(\"\\nText paths:\")\nfor lbl_path in labels_paths:\n    print(lbl_path)\n#--------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-04-19T12:23:47.901024Z","iopub.execute_input":"2024-04-19T12:23:47.901399Z","iopub.status.idle":"2024-04-19T12:28:42.864155Z","shell.execute_reply.started":"2024-04-19T12:23:47.901369Z","shell.execute_reply":"2024-04-19T12:28:42.863147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#-- Copy all images and labels from input to drone_ds folder ----------------------------\n\ntags = ['train', 'valid', 'test']\n\n#-- copy images --\nfor img_path in images_paths:\n    if 'train' in img_path or 'test' in img_path or 'valid' in img_path:\n        if 'train' in img_path: \n            tag = 'train'\n\n        elif 'valid' in img_path: \n            tag = 'valid'\n\n        elif 'test' in img_path: \n            tag = 'test'\n         \n        \n        #-- log --\n        print(f'copy {tag} images from {img_path} ---- ')\n\n        #-- set source and destination --\n        source_dir = img_path\n        dest_dir = dir_dict[(tag,'images')]\n\n        #-- copy images --\n        files = os.listdir(source_dir)            \n        for file in files:\n            source_file_path = os.path.join(source_dir, file)\n            destination_file_path = os.path.join(dest_dir, file)\n            shutil.copy(source_file_path, destination_file_path)\n\n\n\n#-- copy labels --\nfor lbl_path in labels_paths:\n    if 'train' in lbl_path or 'test' in lbl_path or 'valid' in lbl_path:\n        if 'train' in lbl_path: \n            tag = 'train'\n\n        elif 'valid' in lbl_path: \n            tag = 'valid'\n\n        elif 'test' in lbl_path: \n            tag = 'test'\n        \n            \n        #-- log --\n        print(f'copy {tag} labels from {lbl_path} ---- ')\n\n        #-- set source and destination --\n        source_dir = lbl_path\n        dest_dir = dir_dict[(tag,'labels')]\n\n        #-- copy images --\n        files = os.listdir(source_dir)            \n        for file in files:\n            source_file_path = os.path.join(source_dir, file)\n            destination_file_path = os.path.join(dest_dir, file)\n            shutil.copy(source_file_path, destination_file_path)\n#--------------------------------------------------------------------------------------    ","metadata":{"execution":{"iopub.status.busy":"2024-04-19T12:31:43.283658Z","iopub.execute_input":"2024-04-19T12:31:43.284084Z","iopub.status.idle":"2024-04-19T13:06:58.481743Z","shell.execute_reply.started":"2024-04-19T12:31:43.284054Z","shell.execute_reply":"2024-04-19T13:06:58.480927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os \nimport random\nimport shutil\n\nrandom.seed(0)\n\ntags = ['train', 'valid', 'test']\nimage_divide_path = []\nlabel_divide_path = []\n\n#         if 'train' not in img and 'test' not in img and 'valid' not in img:\n#         if 'train' not in lbl and 'test' not in lbl and 'valid' not in lbl:\n\n#-- copy images --\nfor img_path in images_paths:\n    if 'train' not in img_path and 'test' not in img_path and 'valid' not in img_path:\n        print(img_path)\n        images = os.listdir(img_path)\n        print(len(images))\n        for img in images:\n            image_path = os.path.join(img_path, img)\n            image_divide_path.append(image_path)\n        \nfor lbl_path in labels_paths:\n    if 'train' not in lbl_path and 'test' not in lbl_path and 'valid' not in lbl_path:\n        print(lbl_path)\n        labels = os.listdir(lbl_path)\n        print(len(labels))\n        for lbl in labels:\n            label_path = os.path.join(lbl_path, lbl)\n            label_divide_path.append(label_path)\n# Shuffle the list to randomize the order\n\nrandom.shuffle(image_divide_path)\n\n# Calculate the number of files for each group\ntotal_files = len(image_divide_path)\ntrain_count = int(total_files * 0.70)\nvalid_count = int(total_files * 0.20)\ntest_count = total_files - train_count - valid_count\n\nimage_label_map = {}\nfor image_path in image_divide_path:\n    image_name = os.path.splitext(os.path.basename(image_path))[0]\n    for label_path in label_divide_path:\n        label_name = os.path.splitext(os.path.basename(label_path))[0]\n        if image_name == label_name:\n            image_label_map[image_path] = label_path\n            break\n# print(\"dictionary made\")\n# Use the image_label_map to copy files to the appropriate folders\nfor i, image_path in enumerate(image_divide_path):\n    if i < train_count:\n        dest_dir = dir_dict[('train','images')]\n        dest_dir_lbl = dir_dict[('train','labels')]\n    elif i < train_count + valid_count:\n        dest_dir = dir_dict[('valid','images')]\n        dest_dir_lbl = dir_dict[('valid','labels')]\n    else:\n        dest_dir = dir_dict[('test','images')]\n        dest_dir_lbl = dir_dict[('test','labels')]\n\n    shutil.copy(image_path, dest_dir)\n    shutil.copy(image_label_map[image_path], dest_dir_lbl)\n# print(\"the directories made\")","metadata":{"execution":{"iopub.status.busy":"2024-04-19T13:09:47.902938Z","iopub.execute_input":"2024-04-19T13:09:47.903643Z","iopub.status.idle":"2024-04-19T13:37:36.592586Z","shell.execute_reply.started":"2024-04-19T13:09:47.903613Z","shell.execute_reply":"2024-04-19T13:37:36.591665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### end collection","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport re\n# import pydicom\nimport warnings\n\nfrom PIL import Image\nimport cv2\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\n\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom matplotlib import pyplot as plt\nimport random\npaddingSize= 0\n\nwarnings.filterwarnings(\"ignore\")\n\n\n# DIR_INPUT = '/kaggle/working/Drone-1'\nDIR_INPUT = '/kaggle/working/drone_ds'\nDIR_TRAIN = f'{DIR_INPUT}/train'\nDIR_VALID = f'{DIR_INPUT}/valid'\nDIR_TEST = f'{DIR_INPUT}/test'\n\n\n","metadata":{"papermill":{"duration":8.366111,"end_time":"2024-03-01T22:38:57.348740","exception":false,"start_time":"2024-03-01T22:38:48.982629","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-19T13:37:50.473980Z","iopub.execute_input":"2024-04-19T13:37:50.474435Z","iopub.status.idle":"2024-04-19T13:38:02.122750Z","shell.execute_reply.started":"2024-04-19T13:37:50.474406Z","shell.execute_reply":"2024-04-19T13:38:02.121964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\ndef read_dataset(folder_path):\n  fnames=os.listdir(f\"{folder_path}/images/\")\n  df = pd.DataFrame()\n  filename=[]\n  class_num,xmin,ymin,xmax,ymax=[],[],[],[],[]\n  for fname in fnames:\n    if fname[-3:]== 'jpg'or fname[-3:]== 'JPG' or fname[-3:]== 'png' or fname[-3:]== 'PNG':\n        with open(f\"{folder_path}/labels/{fname[:-3]}txt\") as f:\n          data=f.readline()\n          boxes= data.split()\n          if len(boxes)==5:\n#             if fname=='bird_44.jpg':\n#                 print(\"here\")\n#             else:\n                filename.append(fname)\n                _, x_center, y_center, width, height = map(float,boxes)\n            ##### check for 0 ymax and xmax\n#             if (int((y_center + (height / 2))*640))<=0 or (int((x_center + (width / 2))*640))<=0:\n#               print(data,'file:',fname)\n            #####\n                ymax.append(int((y_center + (height / 2))*640) +1)\n                xmax.append(int((x_center + (width / 2))*640) +1)\n                ymin.append(int((y_center - (height / 2))*640))\n                xmin.append(int((x_center - (width / 2))*640))\n            \n                class_num.append(int(1))\n            \n          else:\n#             print(f\"{filename} dosent contain box data\")\n            \n            filename.append(fname)\n            ymax.append(int(640))\n            xmax.append(int(640))\n            ymin.append(int(0))\n            xmin.append(int(0))\n            class_num.append(int(0))\n        \n    elif fname[-4:]== 'jpeg' or fname[-4:]== 'JPEG':\n        with open(f\"{folder_path}/labels/{fname[:-4]}txt\") as f:\n          data=f.readline()\n          boxes= data.split()\n          if len(boxes)==5:\n            filename.append(fname)\n            _, x_center, y_center, width, height = map(float,boxes)\n            ##### check for 0 ymax and xmax\n#             if (int((y_center + (height / 2))*640))<=0 or (int((x_center + (width / 2))*640))<=0:\n#               print(data,'file:',fname)\n            ######\n            ymax.append(int((y_center + (height / 2))*640) +1)\n            xmax.append(int((x_center + (width / 2))*640) +1)\n            ymin.append(int((y_center - (height / 2))*640))\n            xmin.append(int((x_center - (width / 2))*640))\n            \n            class_num.append(int(1))\n\n          else:\n#             print(f\"{filename} dosent contain box data\")\n            \n            filename.append(fname)\n            ymax.append(int(640))\n            xmax.append(int(640))\n            ymin.append(int(0))\n            xmin.append(int(0))\n            class_num.append(int(0))\n    else :\n#         continue\n        print(fname[-5:])\n##_______\n\n##_______\n        \n        \n\n  df['filename']=filename\n  df['ymax']=ymax\n  df['ymin']=ymin\n  df['xmax']=xmax\n  df['xmin']=xmin\n####\n  df['class_id']=class_num\n    \n  return df\n# testing_df = read_dataset(DIR_TRAIN)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T13:38:07.167438Z","iopub.execute_input":"2024-04-19T13:38:07.168371Z","iopub.status.idle":"2024-04-19T13:38:07.185575Z","shell.execute_reply.started":"2024-04-19T13:38:07.168336Z","shell.execute_reply":"2024-04-19T13:38:07.184586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df = pd.read_csv(f'{DIR_TRAIN}/_annotations.csv') #[:100]\ntrain_df = read_dataset(DIR_TRAIN) #[:30]\nprint(\"df Shape: \"+str(train_df.shape))\n# print(\"No Of Classes: \"+str(train_df[\"class\"].nunique()))\n# train_df['class_id']=1\ntrain_df.sort_values(by='filename').head(10)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T13:38:12.116578Z","iopub.execute_input":"2024-04-19T13:38:12.117419Z","iopub.status.idle":"2024-04-19T13:38:16.400320Z","shell.execute_reply.started":"2024-04-19T13:38:12.117385Z","shell.execute_reply":"2024-04-19T13:38:16.399307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df = pd.read_csv(f'{DIR_TRAIN}/_annotations.csv') #[:100]\n# print(\"df Shape: \"+str(train_df.shape))\n# print(\"No Of Classes: \"+str(train_df[\"class\"].nunique()))\n# train_df['class_id']=1\n# train_df.sort_values(by='filename').head(10)","metadata":{"papermill":{"duration":0.170571,"end_time":"2024-03-01T22:38:57.569734","exception":false,"start_time":"2024-03-01T22:38:57.399163","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-25T15:35:42.749210Z","iopub.execute_input":"2024-03-25T15:35:42.750058Z","iopub.status.idle":"2024-03-25T15:35:42.840772Z","shell.execute_reply.started":"2024-03-25T15:35:42.750023Z","shell.execute_reply":"2024-03-25T15:35:42.839847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# valid_df = pd.read_csv(f'{DIR_VALID}/_annotations.csv')#[:100]\nvalid_df = read_dataset(DIR_VALID) #[:10]\nprint(\"df Shape: \"+str(valid_df.shape))\n# print(\"No Of Classes: \"+str(valid_df[\"class\"].nunique()))\n# valid_df['class_id']=1\nvalid_df.sort_values(by='filename').head(10)","metadata":{"papermill":{"duration":0.082572,"end_time":"2024-03-01T22:38:57.711810","exception":false,"start_time":"2024-03-01T22:38:57.629238","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-19T13:38:32.367630Z","iopub.execute_input":"2024-04-19T13:38:32.368510Z","iopub.status.idle":"2024-04-19T13:38:32.820390Z","shell.execute_reply.started":"2024-04-19T13:38:32.368476Z","shell.execute_reply":"2024-04-19T13:38:32.819348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Thanks -  https://www.kaggle.com/pestipeti/\nclass VinBigDataset(Dataset): #Class to load Training Data\n\n    def __init__(self, dataframe, image_dir, transforms=None,stat = 'Train'):\n        super().__init__()\n\n        self.image_ids = dataframe[\"filename\"].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n        self.stat = stat\n\n    def __getitem__(self, index):\n        if self.stat == 'Train':\n\n            image_id = self.image_ids[index]\n            records = self.df[(self.df['filename'] == image_id)]\n            records = records.reset_index(drop=True)\n\n            # dicom = pydicom.dcmread(f\"{self.image_dir}/{image_id}.dicom\")\n\n            # image = dicom.pixel_array\n\n            # if \"PhotometricInterpretation\" in dicom:\n            #     if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n            #         image = np.amax(image) - image\n\n            # intercept = dicom.RescaleIntercept if \"RescaleIntercept\" in dicom else 0.0\n            # slope = dicom.RescaleSlope if \"RescaleSlope\" in dicom else 1.0\n\n            # if slope != 1:\n            #     image = slope * image.astype(np.float64)\n            #     image = image.astype(np.int16)\n\n\n            # image += np.int16(intercept)\n\n            # image = np.stack([image, image, image])\n            # image = image.astype('float32')\n            # image = image - image.min()\n            # image = image / image.max()\n            # image = image * 255.0\n            # image = image.transpose(1,2,0)\n            image = cv2.imread(f\"{self.image_dir}/images/{image_id}\")\n\n            try:         \n                image = cv2.resize(image,(640,640))\n                image = image / 255.0\n            except Exception as e:\n                width, height = 640, 640\n                background_color = (0.0, 0.0, 0.0)  # White\n\n                # Create a blank white image\n                image = np.full((height, width, 3), background_color, dtype=np.uint8)\n                print(f\"{image_id}:{e}\")#, image)\n#                 continue\n                \n#             image = cv2.resize(image,(640,640))\n#             image = image / 255.0\n\n            # if records.loc[0, \"class_id\"] == 0:\n            #     records = records.loc[[0], :]\n\n            boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n            area = torch.as_tensor(area, dtype=torch.float32)\n            labels = torch.tensor(records[\"class_id\"].values, dtype=torch.int64)\n\n            # suppose all instances are not crowd\n            iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n\n            target = {}\n            target['boxes'] = boxes\n            target['labels'] = labels\n            # target['image_id'] = torch.tensor([index])\n            # target['area'] = area\n            # target['iscrowd'] = iscrowd\n\n            if self.transforms:\n                sample = {\n                    'image': image,\n                    'bboxes': target['boxes'],\n                    'labels': labels\n                }\n                sample = self.transforms(**sample)\n                image = sample['image']\n\n                target['boxes'] = torch.tensor(sample['bboxes'])\n\n            if target[\"boxes\"].shape[0] == 0:\n                # Albumentation cuts the target (class 14, 1x1px in the corner)\n                target[\"boxes\"] = torch.from_numpy(np.array([[0.0, 0.0, 1.0, 1.0]]))\n                # target[\"area\"] = torch.tensor([1.0], dtype=torch.float32)\n                target[\"labels\"] = torch.tensor([0], dtype=torch.int64)\n\n            return image, target, image_id\n\n        else:\n\n            image_id = self.image_ids[index]\n            records = self.df[(self.df['filename'] == image_id)]\n            records = records.reset_index(drop=True)\n\n            # dicom = pydicom.dcmread(f\"{self.image_dir}/{image_id}.dicom\")\n\n            # image = dicom.pixel_array\n\n            # intercept = dicom.RescaleIntercept if \"RescaleIntercept\" in dicom else 0.0\n            # slope = dicom.RescaleSlope if \"RescaleSlope\" in dicom else 1.0\n\n            # if slope != 1:\n            #     image = slope * image.astype(np.float64)\n            #     image = image.astype(np.int16)\n\n            # image += np.int16(intercept)\n\n            # image = np.stack([image, image, image])\n            # image = image.astype('float32')\n            # image = image - image.min()\n            # image = image / image.max()\n            # image = image * 255.0\n            # image = image.transpose(1,2,0)\n            image = cv2.imread(f\"{self.image_dir}/images/{image_id}\")\n\n            try:\n                image = cv2.resize(image,(640,640))\n                image = image / 255.0\n            except Exception as e:\n                width, height = 640, 640\n                background_color = (0.0, 0.0, 0.0)  # White\n\n                # Create a blank white image\n                image = np.full((height, width, 3), background_color, dtype=np.uint8)\n                print(f\"{image_id}:{e}\")#, image)\n                \n\n            if self.transforms:\n                sample = {\n                    'image': image,\n                }\n                sample = self.transforms(**sample)\n                image = sample['image']\n\n            return image, image_id\n\n    def __len__(self):\n        return self.image_ids.shape[0]","metadata":{"papermill":{"duration":0.078777,"end_time":"2024-03-01T22:38:57.851199","exception":false,"start_time":"2024-03-01T22:38:57.772422","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-19T13:38:36.563143Z","iopub.execute_input":"2024-04-19T13:38:36.563638Z","iopub.status.idle":"2024-04-19T13:38:36.583920Z","shell.execute_reply.started":"2024-04-19T13:38:36.563609Z","shell.execute_reply":"2024-04-19T13:38:36.582970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torchvision\n# help(torchvision.models.detection.FasterRCNN)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:05:28.628053Z","iopub.execute_input":"2024-03-24T09:05:28.628465Z","iopub.status.idle":"2024-03-24T09:05:28.646922Z","shell.execute_reply.started":"2024-03-24T09:05:28.628434Z","shell.execute_reply":"2024-03-24T09:05:28.645817Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torchvision\n# dir(torchvision.models.detection)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:04:55.077758Z","iopub.execute_input":"2024-03-24T09:04:55.078120Z","iopub.status.idle":"2024-03-24T09:04:55.085185Z","shell.execute_reply.started":"2024-03-24T09:04:55.078093Z","shell.execute_reply":"2024-03-24T09:04:55.084193Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install torchsummary\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T12:28:48.841158Z","iopub.execute_input":"2024-03-26T12:28:48.841951Z","iopub.status.idle":"2024-03-26T12:29:06.801450Z","shell.execute_reply.started":"2024-03-26T12:28:48.841920Z","shell.execute_reply":"2024-03-26T12:29:06.800198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\n\n\n# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\nmodel = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n\n# from torchinfo import summary\n# summary(model)\n# model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained = True)\n\n\n# 'fasterrcnn_mobilenet_v3_large_320_fpn',\n# 'fasterrcnn_mobilenet_v3_large_fpn',\n# 'fasterrcnn_resnet50_fpn',\n# 'fasterrcnn_resnet50_fpn_v2","metadata":{"papermill":{"duration":2.40578,"end_time":"2024-03-01T22:39:00.310756","exception":false,"start_time":"2024-03-01T22:38:57.904976","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-19T13:38:43.356494Z","iopub.execute_input":"2024-04-19T13:38:43.357347Z","iopub.status.idle":"2024-04-19T13:38:44.613269Z","shell.execute_reply.started":"2024-04-19T13:38:43.357315Z","shell.execute_reply":"2024-04-19T13:38:44.612251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### VGG16\n# from torchvision.models.detection import FasterRCNN\n# from torchvision.models.detection.rpn import AnchorGenerator\n# backbone = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT).features\n# backbone.out_channels = 512\n# anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),aspect_ratios=((0.5, 1.0, 2.0),))\n# roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],output_size=7, sampling_ratio=2)\n#  # put the pieces together inside a FasterRCNN model\n# model = FasterRCNN(backbone,num_classes=2,rpn_anchor_generator=anchor_generator,box_roi_pool=roi_pooler)\n# model","metadata":{"execution":{"iopub.status.busy":"2024-04-02T12:32:11.332439Z","iopub.execute_input":"2024-04-02T12:32:11.332790Z","iopub.status.idle":"2024-04-02T12:32:17.129449Z","shell.execute_reply.started":"2024-04-02T12:32:11.332763Z","shell.execute_reply":"2024-04-02T12:32:17.128552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 2 # \n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor =  FastRCNNPredictor(in_features,num_classes)\n# replace the pre-trained head with a new one\n# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"papermill":{"duration":0.059982,"end_time":"2024-03-01T22:39:00.425140","exception":false,"start_time":"2024-03-01T22:39:00.365158","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-19T13:38:47.771322Z","iopub.execute_input":"2024-04-19T13:38:47.771715Z","iopub.status.idle":"2024-04-19T13:38:47.777559Z","shell.execute_reply.started":"2024-04-19T13:38:47.771685Z","shell.execute_reply":"2024-04-19T13:38:47.776701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = VinBigDataset(train_df, DIR_TRAIN, ToTensorV2(p=1.0))#, get_train_transform())\nvalid_dataset = VinBigDataset(valid_df, DIR_VALID, ToTensorV2(p=1.0))#, get_valid_transform())\n\n\n# split the dataset in train and test set\nindices = torch.randperm(len(train_dataset)).tolist()\n# Create train and validate data loader\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size= 8 , # 8\n    shuffle=True,\n    num_workers=0,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size= 8,# 8\n    shuffle=False,\n    num_workers=0,\n    collate_fn=collate_fn\n)","metadata":{"papermill":{"duration":0.142132,"end_time":"2024-03-01T22:39:00.619260","exception":false,"start_time":"2024-03-01T22:39:00.477128","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-19T13:38:51.251556Z","iopub.execute_input":"2024-04-19T13:38:51.251952Z","iopub.status.idle":"2024-04-19T13:38:51.352129Z","shell.execute_reply.started":"2024-04-19T13:38:51.251921Z","shell.execute_reply":"2024-04-19T13:38:51.351160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_df\ndel valid_df\n# del valid_data_loader","metadata":{"execution":{"iopub.status.busy":"2024-04-02T12:38:23.024012Z","iopub.execute_input":"2024-04-02T12:38:23.024754Z","iopub.status.idle":"2024-04-02T12:38:23.057734Z","shell.execute_reply.started":"2024-04-02T12:38:23.024721Z","shell.execute_reply":"2024-04-02T12:38:23.056437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train dataset sample\nimages, targets, image_ids = next(iter(train_data_loader))\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\nfor number in random.sample([1,2,3],3):\n#   boxes = targets[number]['boxes'].cpu().numpy().astype(np.int32)\n  boxes = (targets[number]['boxes'].cpu().numpy()).astype(np.int32)\n  img = images[number].permute(1,2,0).cpu().numpy()\n  labels= targets[number]['labels'].cpu().numpy().astype(np.int32)\n  fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n  for i in range(len(boxes)):\n      img = cv2.rectangle(img,(boxes[i][0]+paddingSize,boxes[i][1]+paddingSize),(boxes[i][2]+paddingSize,boxes[i][3]+paddingSize),(255,0,0),2)\n      #print(le.inverse_transform([labels[i]-1])[0])\n      #print(label_to_name(labels[i]), (int(boxes[i][0]), int(boxes[i][1])))\n      # img = cv2.putText(img, labels[i], (int(boxes[i][0]), int(boxes[i][1])), cv2.FONT_HERSHEY_TRIPLEX,1, (255,0,0), 2, cv2.LINE_AA)\n\n  ax.set_axis_off()\n  ax.imshow(img)","metadata":{"papermill":{"duration":1.778554,"end_time":"2024-03-01T22:39:02.448964","exception":false,"start_time":"2024-03-01T22:39:00.670410","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-19T13:38:55.965193Z","iopub.execute_input":"2024-04-19T13:38:55.966021Z","iopub.status.idle":"2024-04-19T13:38:58.108792Z","shell.execute_reply.started":"2024-04-19T13:38:55.965989Z","shell.execute_reply":"2024-04-19T13:38:58.107816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total / self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0","metadata":{"papermill":{"duration":0.137379,"end_time":"2024-03-01T22:39:02.664035","exception":false,"start_time":"2024-03-01T22:39:02.526656","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-19T13:39:13.567454Z","iopub.execute_input":"2024-04-19T13:39:13.567824Z","iopub.status.idle":"2024-04-19T13:39:13.574262Z","shell.execute_reply.started":"2024-04-19T13:39:13.567795Z","shell.execute_reply":"2024-04-19T13:39:13.573147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Model\n\n# model = torch.load('/kaggle/input/model-new2/mdl_new2.pth')#,map_location=torch.device('cpu'))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:02:43.344096Z","iopub.execute_input":"2024-03-05T09:02:43.344650Z","iopub.status.idle":"2024-03-05T09:02:45.070123Z","shell.execute_reply.started":"2024-03-05T09:02:43.344607Z","shell.execute_reply":"2024-03-05T09:02:45.069040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n\nnum_epochs = 5  #Low epoch to save GPU time","metadata":{"papermill":{"duration":0.152377,"end_time":"2024-03-01T22:39:02.901167","exception":false,"start_time":"2024-03-01T22:39:02.748790","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-19T13:40:47.438992Z","iopub.execute_input":"2024-04-19T13:40:47.439502Z","iopub.status.idle":"2024-04-19T13:40:47.456001Z","shell.execute_reply.started":"2024-04-19T13:40:47.439473Z","shell.execute_reply":"2024-04-19T13:40:47.455087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def voc_ap(rec, prec, use_07_metric=True):\n    \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:True).\n    \"\"\"\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T13:39:21.148134Z","iopub.execute_input":"2024-04-19T13:39:21.149009Z","iopub.status.idle":"2024-04-19T13:39:21.158205Z","shell.execute_reply.started":"2024-04-19T13:39:21.148976Z","shell.execute_reply":"2024-04-19T13:39:21.157227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_hist = Averager()\nitr = 1\nlossHistoryiter = []\nlossHistoryepoch = []\n\nmAP_Historyepoch = []\n\novthresh=0.5\nuse_07_metric=True\ntp = 0\nfp = 0\nnpos = 0\nTotal_score = 0\nTotal_overlaps = 0\nbest_mAP = 0\n\nimport time\nstart = time.time()\nimport tqdm\nfor epoch in range(num_epochs):\n    loss_hist.reset()\n    \n    model.train()\n\n    for images, targets, image_ids in tqdm.tqdm(train_data_loader):\n\n        images = list(image.float().to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n#         print(type(loss_dict))\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        loss_hist.send(loss_value)\n        lossHistoryiter.append(loss_value)\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        if itr % 50 == 0:\n            print(f\"Iteration #{itr} loss: {loss_value}\",end=\"\\r\")\n        itr += 1\n\n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n    lossHistoryepoch.append(loss_hist.value)\n    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")\n    \n    ## Validation\n    model.eval()\n    cpu_device = torch.device(\"cpu\")\n\n    for images, targets, image_ids in tqdm.tqdm(valid_data_loader):\n        images = list(image.float().to(device) for image in images)\n    #     targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n    #     outputs = model(images)\n\n        with torch.no_grad():\n            outputs = model(images)\n\n        for tar,out in zip(targets,outputs):\n    #         print(tar['boxes'].size(),out['boxes'].size())\n            score = out['scores'].sum().cpu().detach().numpy()\n    #         Total_score = Total_score + score\n\n            bb = tar['boxes'].cpu().detach().numpy()\n            BBGT = out['boxes'].cpu().detach().numpy()\n            ixmin = np.maximum(BBGT[:, 0], bb[0,0])\n            iymin = np.maximum(BBGT[:, 1], bb[0,1])\n            ixmax = np.minimum(BBGT[:, 2], bb[0,2])\n            iymax = np.minimum(BBGT[:, 3], bb[0,3])\n            iw = np.maximum(ixmax - ixmin, 0.)\n            ih = np.maximum(iymax - iymin, 0.)\n            inters = iw * ih\n            uni = ((bb[0,2] - bb[0,0]) * (bb[0,3] - bb[0,1]) +\n                    (BBGT[:, 2] - BBGT[:, 0]) *\n                    (BBGT[:, 3] - BBGT[:, 1]) - inters)\n            overlaps = inters / uni\n    #         print(overlaps)\n\n            if overlaps.size==0:\n                continue\n \n            ovmax = np.max(overlaps)\n            jmax = np.argmax(overlaps)\n            if ovmax > ovthresh:\n                tp+=1\n            else:\n                fp+=1\n            npos+=bb.shape[0]\n    rec = tp / float(npos)\n    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n    ap = voc_ap(rec, prec, use_07_metric)\n    print(f\"Epoch #{epoch} mAP: {ap}\")\n    mAP_Historyepoch.append(ap)\n        \n    # save best validation\n    if best_mAP<ap:\n        torch.save(model, '/kaggle/working/mobilenet_bm_p1.pth')\n        best_mAP = ap\n\nend = time.time()\nhours, rem = divmod(end-start, 3600)\nminutes, seconds = divmod(rem, 60)\nprint(\"Time taken to Train the model :{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n\n\n","metadata":{"papermill":{"duration":34.2718,"end_time":"2024-03-01T22:39:37.250379","exception":false,"start_time":"2024-03-01T22:39:02.978579","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-19T13:58:51.145671Z","iopub.execute_input":"2024-04-19T13:58:51.146436Z","iopub.status.idle":"2024-04-19T13:58:56.245974Z","shell.execute_reply.started":"2024-04-19T13:58:51.146400Z","shell.execute_reply":"2024-04-19T13:58:56.245099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save Model\n\n# torch.save(model, '/kaggle/working/mobilenet_v3_P1.pth')\n","metadata":{"execution":{"iopub.execute_input":"2024-03-01T22:39:37.432735Z","iopub.status.busy":"2024-03-01T22:39:37.432249Z","iopub.status.idle":"2024-03-01T22:39:38.151459Z","shell.execute_reply":"2024-03-01T22:39:38.150528Z"},"papermill":{"duration":0.807161,"end_time":"2024-03-01T22:39:38.154170","exception":false,"start_time":"2024-03-01T22:39:37.347009","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training loss\nimport matplotlib.pyplot as plt\n# x = [i for i in range(num_epochs)]\ny = lossHistoryepoch\n\n# Plot scatter plot of training loss\nx = np.arange(1, num_epochs+1)\ncolors = y  # Use loss values as colors\n\nplt.plot(x, y, marker='o', linestyle='-')#, color=plt.cm.viridis(colors))\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\n# plt.colorbar(label='Loss')\nplt.grid(True)\nplt.show()\n# plt.savefig('plot.jpg')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot validation mAP\nimport matplotlib.pyplot as plt\n# x = [i for i in range(num_epochs)]\nz = mAP_Historyepoch\n\n# Plot scatter plot of training loss\nx = np.arange(1, num_epochs+1)\ncolors = y  # Use loss values as colors\n\nplt.plot(x, z, marker='o', linestyle='-')#, color=plt.cm.viridis(colors))\nplt.xlabel('Epoch')\nplt.ylabel('mAP')\nplt.title('Validation mAP')\n# plt.colorbar(label='Loss')\nplt.grid(True)\nplt.show()\n# plt.savefig('plot.jpg')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DIR_TEST = f'{DIR_INPUT}/test'\n# test_df = pd.read_csv(f'{DIR_TEST}/_annotations.csv') #[:10]\ntest_df = read_dataset(DIR_TEST) \nprint(\"df Shape: \"+str(test_df.shape))\n# print(\"No Of Classes: \"+str(test_df[\"class\"].nunique()))\n# test_df['class_id']=1\ntest_df.sort_values(by='filename').head(10)\n","metadata":{"papermill":{"duration":0.107572,"end_time":"2024-03-01T22:39:39.580796","exception":false,"start_time":"2024-03-01T22:39:39.473224","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-15T15:03:59.745105Z","iopub.execute_input":"2024-04-15T15:03:59.745783Z","iopub.status.idle":"2024-04-15T15:04:00.016794Z","shell.execute_reply.started":"2024-04-15T15:03:59.745742Z","shell.execute_reply":"2024-04-15T15:04:00.015886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# labels =  targets[1]['labels'].cpu().numpy()\nmodel.eval()\ncpu_device = torch.device(\"cpu\")","metadata":{"papermill":{"duration":0.088854,"end_time":"2024-03-01T22:39:39.748180","exception":false,"start_time":"2024-03-01T22:39:39.659326","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-15T15:04:48.300916Z","iopub.execute_input":"2024-04-15T15:04:48.301275Z","iopub.status.idle":"2024-04-15T15:04:48.307836Z","shell.execute_reply.started":"2024-04-15T15:04:48.301248Z","shell.execute_reply":"2024-04-15T15:04:48.306926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = VinBigDataset(test_df, DIR_TEST, ToTensorV2(p=1.0))#,\"Test\")\n\ntest_data_loader = DataLoader(\n    test_dataset,\n    batch_size= 8, # 8\n    shuffle=False,\n    num_workers=0,\n    drop_last=False,\n    collate_fn=collate_fn\n)","metadata":{"papermill":{"duration":0.086948,"end_time":"2024-03-01T22:39:39.913719","exception":false,"start_time":"2024-03-01T22:39:39.826771","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-15T15:04:59.219298Z","iopub.execute_input":"2024-04-15T15:04:59.220227Z","iopub.status.idle":"2024-04-15T15:04:59.226599Z","shell.execute_reply.started":"2024-04-15T15:04:59.220186Z","shell.execute_reply":"2024-04-15T15:04:59.225577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test_df","metadata":{"execution":{"iopub.status.busy":"2024-03-12T07:08:36.084937Z","iopub.execute_input":"2024-03-12T07:08:36.085313Z","iopub.status.idle":"2024-03-12T07:08:36.089765Z","shell.execute_reply.started":"2024-03-12T07:08:36.085283Z","shell.execute_reply":"2024-03-12T07:08:36.088699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_prediction_string(labels, boxes, scores):\n    pred_strings = []\n    for j in zip(labels, scores, boxes):\n        pred_strings.append(\"{0} {1:.4f} {2} {3} {4} {5}\".format(\n            j[0], j[1], j[2][0], j[2][1], j[2][2], j[2][3]))\n\n    return \" \".join(pred_strings)","metadata":{"papermill":{"duration":0.088009,"end_time":"2024-03-01T22:39:40.078707","exception":false,"start_time":"2024-03-01T22:39:39.990698","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-15T15:05:03.577455Z","iopub.execute_input":"2024-04-15T15:05:03.578069Z","iopub.status.idle":"2024-04-15T15:05:03.583672Z","shell.execute_reply.started":"2024-04-15T15:05:03.578035Z","shell.execute_reply":"2024-04-15T15:05:03.582759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test dataset sample\nimages,target, image_ids = next(iter(test_data_loader))\nimages = list(image.to(device) for image in images)\n\nfor number in random.sample([1,2,3],3):\n  img = images[number].permute(1,2,0).cpu().numpy()\n  #labels= targets[number]['labels'].cpu().numpy().astype(np.int32)\n  fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n  ax.set_axis_off()\n  ax.imshow(img)","metadata":{"papermill":{"duration":1.589216,"end_time":"2024-03-01T22:39:41.745029","exception":false,"start_time":"2024-03-01T22:39:40.155813","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-15T15:05:12.538688Z","iopub.execute_input":"2024-04-15T15:05:12.539061Z","iopub.status.idle":"2024-04-15T15:05:13.930039Z","shell.execute_reply.started":"2024-04-15T15:05:12.539031Z","shell.execute_reply":"2024-04-15T15:05:13.929088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Test images\nimages,target, image_ids = next(iter(test_data_loader))\nimages = list(img.float().to(device) for img in images)\n\noutputs = model(images)\noutputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n\n\nboxes = outputs[0]['boxes'].cpu().detach().numpy().astype(np.int32)\nimg = images[0].permute(1,2,0).cpu().detach().numpy()\nlabels= outputs[0]['labels'].cpu().detach().numpy().astype(np.int32)\nscore = outputs[0]['scores']\n# print(score)\n\nfig, ax = plt.subplots(1,1,figsize=(16, 8))\n\nimg = cv2.cvtColor(np.float32(img), cv2.COLOR_RGB2BGR)\nfor i in range(len(boxes)):\n  img = cv2.rectangle(img,(boxes[i][0]+paddingSize,boxes[i][1]+paddingSize),(boxes[i][2]+paddingSize,boxes[i][3]+paddingSize),(255,0,0),2)\n  #print(le.inverse_transform([labels[i]-1])[0])\n#   print(label_to_name(labels[i]), (boxes[i][0]+paddingSize,boxes[i][1]+paddingSize),(boxes[i][2]+paddingSize,boxes[i][3]+paddingSize))\n  # img = cv2.putText(img, label_to_name(labels[i]), (int(boxes[i][0]), int(boxes[i][1])), cv2.FONT_HERSHEY_TRIPLEX,3, (255,0,0), 3, cv2.LINE_AA)\nt_boxes = target[0]['boxes'].cpu().detach().numpy().astype(np.int32)\nfor i in range(len(t_boxes)):\n  img = cv2.rectangle(img,(t_boxes[i][0]+paddingSize,t_boxes[i][1]+paddingSize),(t_boxes[i][2]+paddingSize,t_boxes[i][3]+paddingSize),(0,0,255),2)\n  \nax.set_axis_off()\nax.imshow(img)","metadata":{"papermill":{"duration":1.479842,"end_time":"2024-03-01T22:39:43.322727","exception":false,"start_time":"2024-03-01T22:39:41.842885","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-15T15:05:36.385364Z","iopub.execute_input":"2024-04-15T15:05:36.385849Z","iopub.status.idle":"2024-04-15T15:05:36.906721Z","shell.execute_reply.started":"2024-04-15T15:05:36.385816Z","shell.execute_reply":"2024-04-15T15:05:36.905555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T07:09:11.264343Z","iopub.execute_input":"2024-03-12T07:09:11.264725Z","iopub.status.idle":"2024-03-12T07:09:11.268933Z","shell.execute_reply.started":"2024-03-12T07:09:11.264699Z","shell.execute_reply":"2024-03-12T07:09:11.267976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport tqdm\n\novthresh=0.5\nuse_07_metric=True\ntp = 0\nfp = 0\nnpos = 0\n# def voc_ap(rec, prec, use_07_metric=True):\n#     \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n#     Compute VOC AP given precision and recall.\n#     If use_07_metric is true, uses the\n#     VOC 07 11 point method (default:True).\n#     \"\"\"\n#     if use_07_metric:\n#         # 11 point metric\n#         ap = 0.\n#         for t in np.arange(0., 1.1, 0.1):\n#             if np.sum(rec >= t) == 0:\n#                 p = 0\n#             else:\n#                 p = np.max(prec[rec >= t])\n#             ap = ap + p / 11.\n#     else:\n#         # correct AP calculation\n#         # first append sentinel values at the end\n#         mrec = np.concatenate(([0.], rec, [1.]))\n#         mpre = np.concatenate(([0.], prec, [0.]))\n\n#         # compute the precision envelope\n#         for i in range(mpre.size - 1, 0, -1):\n#             mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n#         # to calculate area under PR curve, look for points\n#         # where X axis (recall) changes value\n#         i = np.where(mrec[1:] != mrec[:-1])[0]\n\n#         # and sum (\\Delta recall) * prec\n#         ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n#     return ap\n\n\n\nTotal_score = 0\nTotal_overlaps = 0\nstart = time.time()\nfor images, targets, image_ids in tqdm.tqdm(test_data_loader):\n    images = list(image.float().to(device) for image in images)\n#     targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n#     outputs = model(images)\n\n    with torch.no_grad():\n        outputs = model(images)\n        \n    for tar,out in zip(targets,outputs):\n#         print(tar['boxes'].size(),out['boxes'].size())\n        score = out['scores'].sum().cpu().detach().numpy()\n#         Total_score = Total_score + score\n        \n        bb = tar['boxes'].cpu().detach().numpy()\n        BBGT = out['boxes'].cpu().detach().numpy()\n        ixmin = np.maximum(BBGT[:, 0], bb[0,0])\n        iymin = np.maximum(BBGT[:, 1], bb[0,1])\n        ixmax = np.minimum(BBGT[:, 2], bb[0,2])\n        iymax = np.minimum(BBGT[:, 3], bb[0,3])\n        iw = np.maximum(ixmax - ixmin, 0.)\n        ih = np.maximum(iymax - iymin, 0.)\n        inters = iw * ih\n        uni = ((bb[0,2] - bb[0,0]) * (bb[0,3] - bb[0,1]) +\n                (BBGT[:, 2] - BBGT[:, 0]) *\n                (BBGT[:, 3] - BBGT[:, 1]) - inters)\n        overlaps = inters / uni\n#         print(overlaps)\n        Total_overlaps = Total_overlaps + overlaps.sum()\n        \n        if overlaps.size==0:\n#             overlaps=[0]\n            continue\n#         else:\n#             continue\n        Total_score = Total_score + score\n        ovmax = np.max(overlaps)\n        jmax = np.argmax(overlaps)\n        if ovmax > ovthresh:\n            tp+=1\n        else:\n            fp+=1\n        npos+=bb.shape[0]\n        \n\nend = time.time()\nhours, rem = divmod(end-start, 3600)\nminutes, seconds = divmod(rem, 60)\nprint(\"Time taken to Test the model :{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n\nrec = tp / float(npos)\nprec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\nap = voc_ap(rec, prec, use_07_metric)\nIOU = Total_overlaps/npos\nconfidence = Total_score/npos","metadata":{"papermill":{"duration":7.697966,"end_time":"2024-03-01T22:39:51.390706","exception":false,"start_time":"2024-03-01T22:39:43.692740","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-15T15:25:48.160276Z","iopub.execute_input":"2024-04-15T15:25:48.161099Z","iopub.status.idle":"2024-04-15T15:29:22.658851Z","shell.execute_reply.started":"2024-04-15T15:25:48.161063Z","shell.execute_reply":"2024-04-15T15:29:22.657660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('mean average precision:',ap)\nprint('confidence :',confidence)\nprint(\"IoU:\", IOU)","metadata":{"papermill":{"duration":0.120741,"end_time":"2024-03-01T22:39:51.623227","exception":false,"start_time":"2024-03-01T22:39:51.502486","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-15T15:30:09.025620Z","iopub.execute_input":"2024-04-15T15:30:09.026434Z","iopub.status.idle":"2024-04-15T15:30:09.031455Z","shell.execute_reply.started":"2024-04-15T15:30:09.026401Z","shell.execute_reply":"2024-04-15T15:30:09.030396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:04:01.085387Z","iopub.execute_input":"2024-03-05T09:04:01.085781Z","iopub.status.idle":"2024-03-05T09:04:01.091017Z","shell.execute_reply.started":"2024-03-05T09:04:01.085751Z","shell.execute_reply":"2024-03-05T09:04:01.089516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n'''\n\n### predict images\n\nfolder_path = '/kaggle/input/imagefortest'\n\n# Get list of files in the folder\nfile_list = os.listdir(folder_path)\n\n# Read each image in the folder\nlist_image = []\nfor file_name in file_list:\n#     if file_name.endswith('.jpg') or file_name.endswith('.png'):\n        image_path = os.path.join(folder_path, file_name)\n#         print(image_path)\n        image = cv2.imread(image_path)\n        if image is not None:\n            list_image.append(image)\n# print(len(list_image))\ncounter = 1\nprint(len(list_image))\nfor images in tqdm.tqdm(list_image):\n    image = torch.tensor(images).to(device)\n    image = image/255.0\n    image = image.unsqueeze(0)\n    image = image.permute(0,3,1,2)\n#     image = image.squeeze(0)\n    \n#     images.to(device)\n    start = time.time()\n#     outputs = model(image)\n    \n    with torch.no_grad():\n        outputs = model(image)\n    print(len(outputs[0]))\n    outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n    end = time.time()\n    \n    hours, rem = divmod(end-start, 3600)\n    minutes, seconds = divmod(rem, 60)\n    print('Number :',counter )\n    print(\"Time taken to per image the model :{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n\n    boxes = outputs[0]['boxes'].cpu().detach().numpy().astype(np.int32)\n    img = image.permute(0,2,3,1).cpu().detach().numpy()\n    img = img.squeeze(0)\n    labels= outputs[0]['labels'].cpu().detach().numpy().astype(np.int32)\n    score = outputs[0]['scores'].cpu().detach().numpy()\n    print('label:',labels ,'-  score:', score)\n    \n\n    fig, ax = plt.subplots(1, 1)#, figsize=(8, 5))\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    img = cv2.cvtColor(np.float32(img), cv2.COLOR_RGB2BGR)\n    for i in range(len(boxes)):\n      if score[i]>0.20: \n        img = cv2.rectangle(img,(boxes[i][0]+paddingSize,boxes[i][1]+paddingSize),(boxes[i][2]+paddingSize,boxes[i][3]+paddingSize),(0,255,0),2)\n        img = cv2.putText(img,str(score[i]),(boxes[i][0]+paddingSize,boxes[i][1]+paddingSize-10),font,0.5,(0,255,0),1)\n\n    ax.set_axis_off()\n    ax.imshow(img)\n    fig.savefig(f'{counter}.jpeg',dpi = 600)\n    \n    counter = counter + 1\n    \n    \n'''","metadata":{"papermill":{"duration":1.479842,"end_time":"2024-03-01T22:39:43.322727","exception":false,"start_time":"2024-03-01T22:39:41.842885","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T10:02:28.140762Z","iopub.execute_input":"2024-03-05T10:02:28.141206Z","iopub.status.idle":"2024-03-05T10:02:56.101388Z","shell.execute_reply.started":"2024-03-05T10:02:28.141170Z","shell.execute_reply":"2024-03-05T10:02:56.100032Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
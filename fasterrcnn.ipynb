{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7849554,"sourceType":"datasetVersion","datasetId":4603064},{"sourceId":7849928,"sourceType":"datasetVersion","datasetId":4603334}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# dataset load\n!pip install roboflow\n\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"8CzHJJr0mbSZN5Yndqlp\")\nproject = rf.workspace(\"project-986i8\").project(\"drone-uskpc\")\ndataset = project.version(1).download(\"tensorflow\")\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":70.064001,"end_time":"2024-03-01T22:38:48.933196","exception":false,"start_time":"2024-03-01T22:37:38.869195","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-13T19:45:09.407009Z","iopub.execute_input":"2024-03-13T19:45:09.407968Z","iopub.status.idle":"2024-03-13T19:46:35.588831Z","shell.execute_reply.started":"2024-03-13T19:45:09.407930Z","shell.execute_reply":"2024-03-13T19:46:35.587753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport re\n# import pydicom\nimport warnings\n\nfrom PIL import Image\nimport cv2\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\n\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom matplotlib import pyplot as plt\nimport random\npaddingSize= 0\n\nwarnings.filterwarnings(\"ignore\")\n\n\nDIR_INPUT = '/kaggle/working/Drone-1'\nDIR_TRAIN = f'{DIR_INPUT}/train'\n# DIR_VALID = f'{DIR_INPUT}/valid'\nDIR_TEST = f'{DIR_INPUT}/test'","metadata":{"papermill":{"duration":8.366111,"end_time":"2024-03-01T22:38:57.348740","exception":false,"start_time":"2024-03-01T22:38:48.982629","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-13T19:46:48.927539Z","iopub.execute_input":"2024-03-13T19:46:48.927900Z","iopub.status.idle":"2024-03-13T19:46:58.076317Z","shell.execute_reply.started":"2024-03-13T19:46:48.927870Z","shell.execute_reply":"2024-03-13T19:46:58.075524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(f'{DIR_TRAIN}/_annotations.csv')##[:100]\nprint(\"df Shape: \"+str(train_df.shape))\nprint(\"No Of Classes: \"+str(train_df[\"class\"].nunique()))\ntrain_df['class_id']=1\ntrain_df.sort_values(by='filename').head(10)","metadata":{"papermill":{"duration":0.170571,"end_time":"2024-03-01T22:38:57.569734","exception":false,"start_time":"2024-03-01T22:38:57.399163","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-13T19:51:39.181073Z","iopub.execute_input":"2024-03-13T19:51:39.181802Z","iopub.status.idle":"2024-03-13T19:51:39.225125Z","shell.execute_reply.started":"2024-03-13T19:51:39.181769Z","shell.execute_reply":"2024-03-13T19:51:39.224296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# valid_df = pd.read_csv(f'{DIR_VALID}/_annotations.csv')#[:100]\n# print(\"df Shape: \"+str(valid_df.shape))\n# print(\"No Of Classes: \"+str(valid_df[\"class\"].nunique()))\n# valid_df['class_id']=1\n# valid_df.sort_values(by='filename').head(10)","metadata":{"papermill":{"duration":0.082572,"end_time":"2024-03-01T22:38:57.711810","exception":false,"start_time":"2024-03-01T22:38:57.629238","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-12T07:03:30.199481Z","iopub.execute_input":"2024-03-12T07:03:30.199832Z","iopub.status.idle":"2024-03-12T07:03:30.223190Z","shell.execute_reply.started":"2024-03-12T07:03:30.199802Z","shell.execute_reply":"2024-03-12T07:03:30.222364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Thanks -  https://www.kaggle.com/pestipeti/\nclass VinBigDataset(Dataset): #Class to load Training Data\n\n    def __init__(self, dataframe, image_dir, transforms=None,stat = 'Train'):\n        super().__init__()\n\n        self.image_ids = dataframe[\"filename\"].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n        self.stat = stat\n\n    def __getitem__(self, index):\n        if self.stat == 'Train':\n\n            image_id = self.image_ids[index]\n            records = self.df[(self.df['filename'] == image_id)]\n            records = records.reset_index(drop=True)\n\n            # dicom = pydicom.dcmread(f\"{self.image_dir}/{image_id}.dicom\")\n\n            # image = dicom.pixel_array\n\n            # if \"PhotometricInterpretation\" in dicom:\n            #     if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n            #         image = np.amax(image) - image\n\n            # intercept = dicom.RescaleIntercept if \"RescaleIntercept\" in dicom else 0.0\n            # slope = dicom.RescaleSlope if \"RescaleSlope\" in dicom else 1.0\n\n            # if slope != 1:\n            #     image = slope * image.astype(np.float64)\n            #     image = image.astype(np.int16)\n\n\n            # image += np.int16(intercept)\n\n            # image = np.stack([image, image, image])\n            # image = image.astype('float32')\n            # image = image - image.min()\n            # image = image / image.max()\n            # image = image * 255.0\n            # image = image.transpose(1,2,0)\n            image = cv2.imread(f\"{self.image_dir}/{image_id}\")\n            image = image / 255.0\n\n            # if records.loc[0, \"class_id\"] == 0:\n            #     records = records.loc[[0], :]\n\n            boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n            area = torch.as_tensor(area, dtype=torch.float32)\n            labels = torch.tensor(records[\"class_id\"].values, dtype=torch.int64)\n\n            # suppose all instances are not crowd\n            iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n\n            target = {}\n            target['boxes'] = boxes\n            target['labels'] = labels\n            # target['image_id'] = torch.tensor([index])\n            # target['area'] = area\n            # target['iscrowd'] = iscrowd\n\n            if self.transforms:\n                sample = {\n                    'image': image,\n                    'bboxes': target['boxes'],\n                    'labels': labels\n                }\n                sample = self.transforms(**sample)\n                image = sample['image']\n\n                target['boxes'] = torch.tensor(sample['bboxes'])\n\n            if target[\"boxes\"].shape[0] == 0:\n                # Albumentation cuts the target (class 14, 1x1px in the corner)\n                target[\"boxes\"] = torch.from_numpy(np.array([[0.0, 0.0, 1.0, 1.0]]))\n                # target[\"area\"] = torch.tensor([1.0], dtype=torch.float32)\n                target[\"labels\"] = torch.tensor([0], dtype=torch.int64)\n\n            return image, target, image_id\n\n        else:\n\n            image_id = self.image_ids[index]\n            records = self.df[(self.df['filename'] == image_id)]\n            records = records.reset_index(drop=True)\n\n            # dicom = pydicom.dcmread(f\"{self.image_dir}/{image_id}.dicom\")\n\n            # image = dicom.pixel_array\n\n            # intercept = dicom.RescaleIntercept if \"RescaleIntercept\" in dicom else 0.0\n            # slope = dicom.RescaleSlope if \"RescaleSlope\" in dicom else 1.0\n\n            # if slope != 1:\n            #     image = slope * image.astype(np.float64)\n            #     image = image.astype(np.int16)\n\n            # image += np.int16(intercept)\n\n            # image = np.stack([image, image, image])\n            # image = image.astype('float32')\n            # image = image - image.min()\n            # image = image / image.max()\n            # image = image * 255.0\n            # image = image.transpose(1,2,0)\n            image = cv2.imread(f\"{self.image_dir}/{image_id}\")\n            image = image / 255.0\n\n            if self.transforms:\n                sample = {\n                    'image': image,\n                }\n                sample = self.transforms(**sample)\n                image = sample['image']\n\n            return image, image_id\n\n    def __len__(self):\n        return self.image_ids.shape[0]","metadata":{"papermill":{"duration":0.078777,"end_time":"2024-03-01T22:38:57.851199","exception":false,"start_time":"2024-03-01T22:38:57.772422","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-13T19:51:43.373604Z","iopub.execute_input":"2024-03-13T19:51:43.374044Z","iopub.status.idle":"2024-03-13T19:51:43.391666Z","shell.execute_reply.started":"2024-03-13T19:51:43.374011Z","shell.execute_reply":"2024-03-13T19:51:43.390747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n\n\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# loadmodel = model.to(device)\n# from torchinfo import summary\n# summary(loadmodel)\n# num_ftrs = model.fc.in_features\n# backbone = model.backbone\n# print(backbone)\n# model = backbone.fpn\n# print(fc_layers)\n# model.fc = nn.Linear(num_ftrs, 2)\n# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"papermill":{"duration":2.40578,"end_time":"2024-03-01T22:39:00.310756","exception":false,"start_time":"2024-03-01T22:38:57.904976","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-13T19:47:14.553585Z","iopub.execute_input":"2024-03-13T19:47:14.553933Z","iopub.status.idle":"2024-03-13T19:47:16.620239Z","shell.execute_reply.started":"2024-03-13T19:47:14.553904Z","shell.execute_reply":"2024-03-13T19:47:16.619436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# num_classes = 2 # \n\n# # get number of input features for the classifier\n# in_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# # replace the pre-trained head with a new one\n# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"papermill":{"duration":0.059982,"end_time":"2024-03-01T22:39:00.425140","exception":false,"start_time":"2024-03-01T22:39:00.365158","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-13T19:51:49.601158Z","iopub.execute_input":"2024-03-13T19:51:49.601516Z","iopub.status.idle":"2024-03-13T19:51:49.607416Z","shell.execute_reply.started":"2024-03-13T19:51:49.601489Z","shell.execute_reply":"2024-03-13T19:51:49.606453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = VinBigDataset(train_df, DIR_TRAIN, ToTensorV2(p=1.0))#, get_train_transform())\n# valid_dataset = VinBigDataset(valid_df, DIR_TRAIN, ToTensorV2(p=1.0))#, get_valid_transform())\n\n\n# split the dataset in train and test set\nindices = torch.randperm(len(train_dataset)).tolist()\n# Create train and validate data loader\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size= 8 , # 8\n    shuffle=True,\n    num_workers=0,\n    collate_fn=collate_fn\n)\n\n# valid_data_loader = DataLoader(\n#     valid_dataset,\n#     batch_size=4,# 8\n#     shuffle=False,\n#     num_workers=0,\n#     collate_fn=collate_fn\n# )","metadata":{"papermill":{"duration":0.142132,"end_time":"2024-03-01T22:39:00.619260","exception":false,"start_time":"2024-03-01T22:39:00.477128","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-13T19:51:52.232837Z","iopub.execute_input":"2024-03-13T19:51:52.233435Z","iopub.status.idle":"2024-03-13T19:51:52.241040Z","shell.execute_reply.started":"2024-03-13T19:51:52.233402Z","shell.execute_reply":"2024-03-13T19:51:52.240030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_df\n# del valid_df\n# del valid_data_loader","metadata":{"execution":{"iopub.status.busy":"2024-03-13T19:51:55.925157Z","iopub.execute_input":"2024-03-13T19:51:55.925841Z","iopub.status.idle":"2024-03-13T19:51:55.929591Z","shell.execute_reply.started":"2024-03-13T19:51:55.925806Z","shell.execute_reply":"2024-03-13T19:51:55.928641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train dataset sample\nimages, targets, image_ids = next(iter(train_data_loader))\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\nfor number in random.sample([1,2,3],3):\n  boxes = targets[number]['boxes'].cpu().numpy().astype(np.int32)\n  img = images[number].permute(1,2,0).cpu().numpy()\n  labels= targets[number]['labels'].cpu().numpy().astype(np.int32)\n  fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n  for i in range(len(boxes)):\n      img = cv2.rectangle(img,(boxes[i][0]+paddingSize,boxes[i][1]+paddingSize),(boxes[i][2]+paddingSize,boxes[i][3]+paddingSize),(255,0,0),2)\n      #print(le.inverse_transform([labels[i]-1])[0])\n      #print(label_to_name(labels[i]), (int(boxes[i][0]), int(boxes[i][1])))\n      # img = cv2.putText(img, labels[i], (int(boxes[i][0]), int(boxes[i][1])), cv2.FONT_HERSHEY_TRIPLEX,1, (255,0,0), 2, cv2.LINE_AA)\n\n  ax.set_axis_off()\n  ax.imshow(img)","metadata":{"papermill":{"duration":1.778554,"end_time":"2024-03-01T22:39:02.448964","exception":false,"start_time":"2024-03-01T22:39:00.670410","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-13T19:51:59.677577Z","iopub.execute_input":"2024-03-13T19:51:59.678309Z","iopub.status.idle":"2024-03-13T19:52:01.289968Z","shell.execute_reply.started":"2024-03-13T19:51:59.678277Z","shell.execute_reply":"2024-03-13T19:52:01.289104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total / self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0","metadata":{"papermill":{"duration":0.137379,"end_time":"2024-03-01T22:39:02.664035","exception":false,"start_time":"2024-03-01T22:39:02.526656","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-13T19:52:13.167445Z","iopub.execute_input":"2024-03-13T19:52:13.168053Z","iopub.status.idle":"2024-03-13T19:52:13.174287Z","shell.execute_reply.started":"2024-03-13T19:52:13.168020Z","shell.execute_reply":"2024-03-13T19:52:13.173277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model class must be defined somewhere\n# import torch\n# model = TheModelClass(*args, **kwargs)\n# model.load_state_dict(torch.load('/kaggle/input/model1/mymodel (3).pth'))\n# model.eval()\n# print(model)\n# model = model['model_state_dict']\n# print(model)\nmodel = torch.load('/kaggle/input/model-new2/mdl_new2.pth')#,map_location=torch.device('cpu'))\n# /kaggle/input/model-2/mymodel_2.pth\n# model.eval()\n# model.load_state_dict(checkpoint['model_state_dict'])\n# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n# epoch = checkpoint['epoch']\n# loss = checkpoint['loss']\n\n# model.eval()\n# # - or -\n# model.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:02:43.344096Z","iopub.execute_input":"2024-03-05T09:02:43.344650Z","iopub.status.idle":"2024-03-05T09:02:45.070123Z","shell.execute_reply.started":"2024-03-05T09:02:43.344607Z","shell.execute_reply":"2024-03-05T09:02:45.069040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n\nnum_epochs =  10 #Low epoch to save GPU time","metadata":{"papermill":{"duration":0.152377,"end_time":"2024-03-01T22:39:02.901167","exception":false,"start_time":"2024-03-01T22:39:02.748790","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-13T19:52:20.167326Z","iopub.execute_input":"2024-03-13T19:52:20.168316Z","iopub.status.idle":"2024-03-13T19:52:20.179076Z","shell.execute_reply.started":"2024-03-13T19:52:20.168268Z","shell.execute_reply":"2024-03-13T19:52:20.178351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_hist = Averager()\nitr = 1\nlossHistoryiter = []\nlossHistoryepoch = []\n\nimport time\nstart = time.time()\nimport tqdm\nfor epoch in range(num_epochs):\n    loss_hist.reset()\n\n    for images, targets, image_ids in tqdm.tqdm(train_data_loader):\n\n        images = list(image.float().to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        loss_hist.send(loss_value)\n        lossHistoryiter.append(loss_value)\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        if itr % 50 == 0:\n            print(f\"Iteration #{itr} loss: {loss_value}\")\n\n        itr += 1\n\n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n    lossHistoryepoch.append(loss_hist.value)\n    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")\n\nend = time.time()\nhours, rem = divmod(end-start, 3600)\nminutes, seconds = divmod(rem, 60)\nprint(\"Time taken to Train the model :{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))","metadata":{"papermill":{"duration":34.2718,"end_time":"2024-03-01T22:39:37.250379","exception":false,"start_time":"2024-03-01T22:39:02.978579","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-13T19:52:25.175398Z","iopub.execute_input":"2024-03-13T19:52:25.176325Z","iopub.status.idle":"2024-03-13T19:54:56.933827Z","shell.execute_reply.started":"2024-03-13T19:52:25.176281Z","shell.execute_reply":"2024-03-13T19:54:56.932872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.save(model.state_dict(), '/kaggle/working/model')\n# from pytorch_lightning.callbacks import ModelCheckpoint\n# model.save_checkpoint(\"mymodel.ckpt\")\n# new_model = MyModel.load_from_checkpoint(checkpoint_path=\"example.ckpt\")\ntorch.save(model, '/kaggle/working/mdl_new3.pth')\n","metadata":{"execution":{"iopub.execute_input":"2024-03-01T22:39:37.432735Z","iopub.status.busy":"2024-03-01T22:39:37.432249Z","iopub.status.idle":"2024-03-01T22:39:38.151459Z","shell.execute_reply":"2024-03-01T22:39:38.150528Z"},"papermill":{"duration":0.807161,"end_time":"2024-03-01T22:39:38.154170","exception":false,"start_time":"2024-03-01T22:39:37.347009","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.save({'epoch': 10,\n#             'model_state_dict': model.state_dict(),\n#             'optimizer_state_dict': optimizer.state_dict(),\n#             'loss': lossHistoryepoch}, '/kaggle/working/mymodel.pth')","metadata":{"papermill":{"duration":0.611495,"end_time":"2024-03-01T22:39:38.847244","exception":false,"start_time":"2024-03-01T22:39:38.235749","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-02T03:30:59.158229Z","iopub.execute_input":"2024-03-02T03:30:59.159075Z","iopub.status.idle":"2024-03-02T03:30:59.886141Z","shell.execute_reply.started":"2024-03-02T03:30:59.159043Z","shell.execute_reply":"2024-03-02T03:30:59.885139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training loss\nimport matplotlib.pyplot as plt\n# x = [i for i in range(num_epochs)]\ny = lossHistoryepoch\n# Plot scatter plot of training loss\nx = np.arange(1, num_epochs+1)\ncolors = y  # Use loss values as colors\n\nplt.plot(x, y, marker='o', linestyle='-')#, color=plt.cm.viridis(colors))\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\n# plt.colorbar(label='Loss')\nplt.grid(True)\nplt.show()\n# plt.savefig('plot.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-03-12T07:08:10.620128Z","iopub.execute_input":"2024-03-12T07:08:10.620960Z","iopub.status.idle":"2024-03-12T07:08:10.872164Z","shell.execute_reply.started":"2024-03-12T07:08:10.620927Z","shell.execute_reply":"2024-03-12T07:08:10.871291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import plotly.graph_objects as go\n\n# x = [i for i in range(num_epochs)]\n# y = lossHistoryepoch\n# fig = go.Figure()\n# fig.add_trace(go.Scatter(x=x,y=y,\n#                     mode='lines',\n#                     name='lines'))\n\n# fig.update_layout(title='Loss vs Epochs',\n#                    xaxis_title='Epochs',\n#                    yaxis_title='Loss')\n# fig.show()","metadata":{"execution":{"iopub.execute_input":"2024-03-01T22:39:39.010302Z","iopub.status.busy":"2024-03-01T22:39:39.009942Z","iopub.status.idle":"2024-03-01T22:39:39.393118Z","shell.execute_reply":"2024-03-01T22:39:39.392215Z"},"papermill":{"duration":0.469006,"end_time":"2024-03-01T22:39:39.395161","exception":false,"start_time":"2024-03-01T22:39:38.926155","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DIR_TEST = f'{DIR_INPUT}/test'\ntest_df = pd.read_csv(f'{DIR_TEST}/_annotations.csv')#[:100]\nprint(\"df Shape: \"+str(test_df.shape))\nprint(\"No Of Classes: \"+str(test_df[\"class\"].nunique()))\ntest_df['class_id']=1\ntest_df.sort_values(by='filename').head(10)","metadata":{"papermill":{"duration":0.107572,"end_time":"2024-03-01T22:39:39.580796","exception":false,"start_time":"2024-03-01T22:39:39.473224","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-12T07:08:26.004637Z","iopub.execute_input":"2024-03-12T07:08:26.005511Z","iopub.status.idle":"2024-03-12T07:08:26.028445Z","shell.execute_reply.started":"2024-03-12T07:08:26.005457Z","shell.execute_reply":"2024-03-12T07:08:26.027355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# labels =  targets[1]['labels'].cpu().numpy()\nmodel.eval()\ncpu_device = torch.device(\"cpu\")","metadata":{"papermill":{"duration":0.088854,"end_time":"2024-03-01T22:39:39.748180","exception":false,"start_time":"2024-03-01T22:39:39.659326","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-12T07:08:30.099520Z","iopub.execute_input":"2024-03-12T07:08:30.099963Z","iopub.status.idle":"2024-03-12T07:08:30.110750Z","shell.execute_reply.started":"2024-03-12T07:08:30.099927Z","shell.execute_reply":"2024-03-12T07:08:30.109544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = VinBigDataset(test_df, DIR_TEST, ToTensorV2(p=1.0))#,\"Test\")\n\ntest_data_loader = DataLoader(\n    test_dataset,\n    batch_size= 8, # 8\n    shuffle=False,\n    num_workers=0,\n    drop_last=False,\n    collate_fn=collate_fn\n)","metadata":{"papermill":{"duration":0.086948,"end_time":"2024-03-01T22:39:39.913719","exception":false,"start_time":"2024-03-01T22:39:39.826771","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-12T07:08:33.122362Z","iopub.execute_input":"2024-03-12T07:08:33.122734Z","iopub.status.idle":"2024-03-12T07:08:33.128919Z","shell.execute_reply.started":"2024-03-12T07:08:33.122706Z","shell.execute_reply":"2024-03-12T07:08:33.127894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test_df","metadata":{"execution":{"iopub.status.busy":"2024-03-12T07:08:36.084937Z","iopub.execute_input":"2024-03-12T07:08:36.085313Z","iopub.status.idle":"2024-03-12T07:08:36.089765Z","shell.execute_reply.started":"2024-03-12T07:08:36.085283Z","shell.execute_reply":"2024-03-12T07:08:36.088699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_prediction_string(labels, boxes, scores):\n    pred_strings = []\n    for j in zip(labels, scores, boxes):\n        pred_strings.append(\"{0} {1:.4f} {2} {3} {4} {5}\".format(\n            j[0], j[1], j[2][0], j[2][1], j[2][2], j[2][3]))\n\n    return \" \".join(pred_strings)","metadata":{"papermill":{"duration":0.088009,"end_time":"2024-03-01T22:39:40.078707","exception":false,"start_time":"2024-03-01T22:39:39.990698","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-12T07:08:40.633025Z","iopub.execute_input":"2024-03-12T07:08:40.633353Z","iopub.status.idle":"2024-03-12T07:08:40.639099Z","shell.execute_reply.started":"2024-03-12T07:08:40.633326Z","shell.execute_reply":"2024-03-12T07:08:40.638113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test dataset sample\nimages,target, image_ids = next(iter(test_data_loader))\nimages = list(image.to(device) for image in images)\n\nfor number in random.sample([1,2,3],3):\n  img = images[number].permute(1,2,0).cpu().numpy()\n  #labels= targets[number]['labels'].cpu().numpy().astype(np.int32)\n  fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n  ax.set_axis_off()\n  ax.imshow(img)","metadata":{"papermill":{"duration":1.589216,"end_time":"2024-03-01T22:39:41.745029","exception":false,"start_time":"2024-03-01T22:39:40.155813","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-12T07:08:43.259048Z","iopub.execute_input":"2024-03-12T07:08:43.259789Z","iopub.status.idle":"2024-03-12T07:08:44.498630Z","shell.execute_reply.started":"2024-03-12T07:08:43.259757Z","shell.execute_reply":"2024-03-12T07:08:44.497760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Test images\nimages,target, image_ids = next(iter(test_data_loader))\nimages = list(img.float().to(device) for img in images)\n\noutputs = model(images)\noutputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n\n\nboxes = outputs[0]['boxes'].cpu().detach().numpy().astype(np.int32)\nimg = images[0].permute(1,2,0).cpu().detach().numpy()\nlabels= outputs[0]['labels'].cpu().detach().numpy().astype(np.int32)\nscore = outputs[0]['scores']\n# print(score)\n\nfig, ax = plt.subplots(1,1,figsize=(16, 8))\n\nimg = cv2.cvtColor(np.float32(img), cv2.COLOR_RGB2BGR)\nfor i in range(len(boxes)):\n  img = cv2.rectangle(img,(boxes[i][0]+paddingSize,boxes[i][1]+paddingSize),(boxes[i][2]+paddingSize,boxes[i][3]+paddingSize),(255,0,0),2)\n  #print(le.inverse_transform([labels[i]-1])[0])\n#   print(label_to_name(labels[i]), (boxes[i][0]+paddingSize,boxes[i][1]+paddingSize),(boxes[i][2]+paddingSize,boxes[i][3]+paddingSize))\n  # img = cv2.putText(img, label_to_name(labels[i]), (int(boxes[i][0]), int(boxes[i][1])), cv2.FONT_HERSHEY_TRIPLEX,3, (255,0,0), 3, cv2.LINE_AA)\nt_boxes = target[0]['boxes'].cpu().detach().numpy().astype(np.int32)\nfor i in range(len(t_boxes)):\n  img = cv2.rectangle(img,(t_boxes[i][0]+paddingSize,t_boxes[i][1]+paddingSize),(t_boxes[i][2]+paddingSize,t_boxes[i][3]+paddingSize),(0,0,255),2)\n  \nax.set_axis_off()\nax.imshow(img)","metadata":{"papermill":{"duration":1.479842,"end_time":"2024-03-01T22:39:43.322727","exception":false,"start_time":"2024-03-01T22:39:41.842885","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-12T07:08:56.793947Z","iopub.execute_input":"2024-03-12T07:08:56.794475Z","iopub.status.idle":"2024-03-12T07:08:58.049895Z","shell.execute_reply.started":"2024-03-12T07:08:56.794437Z","shell.execute_reply":"2024-03-12T07:08:58.048913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T07:09:11.264343Z","iopub.execute_input":"2024-03-12T07:09:11.264725Z","iopub.status.idle":"2024-03-12T07:09:11.268933Z","shell.execute_reply.started":"2024-03-12T07:09:11.264699Z","shell.execute_reply":"2024-03-12T07:09:11.267976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport tqdm\n\novthresh=0.5\nuse_07_metric=True\ntp = 0\nfp = 0\nnpos = 0\ndef voc_ap(rec, prec, use_07_metric=True):\n    \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:True).\n    \"\"\"\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\n\nTotal_score = 0\nTotal_overlaps = 0\nstart = time.time()\nfor images, targets, image_ids in tqdm.tqdm(test_data_loader):\n    images = list(image.float().to(device) for image in images)\n#     targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n#     outputs = model(images)\n\n    with torch.no_grad():\n        outputs = model(images)\n        \n    for tar,out in zip(targets,outputs):\n#         print(tar['boxes'].size(),out['boxes'].size())\n        score = out['scores'].sum()\n        Total_score = Total_score + score\n        \n        bb = tar['boxes'].cpu().detach().numpy()\n        BBGT = out['boxes'].cpu().detach().numpy()\n        ixmin = np.maximum(BBGT[:, 0], bb[0,0])\n        iymin = np.maximum(BBGT[:, 1], bb[0,1])\n        ixmax = np.minimum(BBGT[:, 2], bb[0,2])\n        iymax = np.minimum(BBGT[:, 3], bb[0,3])\n        iw = np.maximum(ixmax - ixmin, 0.)\n        ih = np.maximum(iymax - iymin, 0.)\n        inters = iw * ih\n        uni = ((bb[0,2] - bb[0,0]) * (bb[0,3] - bb[0,1]) +\n                (BBGT[:, 2] - BBGT[:, 0]) *\n                (BBGT[:, 3] - BBGT[:, 1]) - inters)\n        overlaps = inters / uni\n#         print(overlaps)\n        Total_overlaps = Total_overlaps + overlaps.sum()\n        \n        if overlaps.size==0:\n#             overlaps=[0]\n            continue\n#         else:\n#             continue\n        ovmax = np.max(overlaps)\n        jmax = np.argmax(overlaps)\n        if ovmax > ovthresh:\n            tp+=1\n        else:\n            fp+=1\n        npos+=bb.shape[0]\n        \n\nend = time.time()\nhours, rem = divmod(end-start, 3600)\nminutes, seconds = divmod(rem, 60)\nprint(\"Time taken to Test the model :{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n\nrec = tp / float(npos)\nprec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\nap = voc_ap(rec, prec, use_07_metric)\nIOU = Total_overlaps/npos\nconfidence = Total_score/npos","metadata":{"papermill":{"duration":7.697966,"end_time":"2024-03-01T22:39:51.390706","exception":false,"start_time":"2024-03-01T22:39:43.692740","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-12T07:09:16.324247Z","iopub.execute_input":"2024-03-12T07:09:16.324590Z","iopub.status.idle":"2024-03-12T07:09:17.164416Z","shell.execute_reply.started":"2024-03-12T07:09:16.324565Z","shell.execute_reply":"2024-03-12T07:09:17.163396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('mean average precision:',ap)\nprint('confidence :',confidence)\nprint(\"IoU:\", IOU)","metadata":{"papermill":{"duration":0.120741,"end_time":"2024-03-01T22:39:51.623227","exception":false,"start_time":"2024-03-01T22:39:51.502486","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-12T07:09:22.706162Z","iopub.execute_input":"2024-03-12T07:09:22.706517Z","iopub.status.idle":"2024-03-12T07:09:22.741290Z","shell.execute_reply.started":"2024-03-12T07:09:22.706490Z","shell.execute_reply":"2024-03-12T07:09:22.740360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:04:01.085387Z","iopub.execute_input":"2024-03-05T09:04:01.085781Z","iopub.status.idle":"2024-03-05T09:04:01.091017Z","shell.execute_reply.started":"2024-03-05T09:04:01.085751Z","shell.execute_reply":"2024-03-05T09:04:01.089516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### predict images\n\nfolder_path = '/kaggle/input/imagefortest'\n\n# Get list of files in the folder\nfile_list = os.listdir(folder_path)\n\n# Read each image in the folder\nlist_image = []\nfor file_name in file_list:\n#     if file_name.endswith('.jpg') or file_name.endswith('.png'):\n        image_path = os.path.join(folder_path, file_name)\n#         print(image_path)\n        image = cv2.imread(image_path)\n        if image is not None:\n            list_image.append(image)\n# print(len(list_image))\ncounter = 1\nprint(len(list_image))\nfor images in tqdm.tqdm(list_image):\n    image = torch.tensor(images).to(device)\n    image = image/255.0\n    image = image.unsqueeze(0)\n    image = image.permute(0,3,1,2)\n#     image = image.squeeze(0)\n    \n#     images.to(device)\n    start = time.time()\n#     outputs = model(image)\n    \n    with torch.no_grad():\n        outputs = model(image)\n    print(len(outputs[0]))\n    outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n    end = time.time()\n    \n    hours, rem = divmod(end-start, 3600)\n    minutes, seconds = divmod(rem, 60)\n    print('Number :',counter )\n    print(\"Time taken to per image the model :{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n\n    boxes = outputs[0]['boxes'].cpu().detach().numpy().astype(np.int32)\n    img = image.permute(0,2,3,1).cpu().detach().numpy()\n    img = img.squeeze(0)\n    labels= outputs[0]['labels'].cpu().detach().numpy().astype(np.int32)\n    score = outputs[0]['scores'].cpu().detach().numpy()\n    print('label:',labels ,'-  score:', score)\n    \n\n    fig, ax = plt.subplots(1, 1)#, figsize=(8, 5))\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    img = cv2.cvtColor(np.float32(img), cv2.COLOR_RGB2BGR)\n    for i in range(len(boxes)):\n      if score[i]>0.60: \n        img = cv2.rectangle(img,(boxes[i][0]+paddingSize,boxes[i][1]+paddingSize),(boxes[i][2]+paddingSize,boxes[i][3]+paddingSize),(0,255,0),2)\n        img = cv2.putText(img,str(score[i]),(boxes[i][0]+paddingSize,boxes[i][1]+paddingSize-10),font,0.5,(0,255,0),1)\n\n    ax.set_axis_off()\n    ax.imshow(img)\n    fig.savefig(f'{counter}.jpeg',dpi = 600)\n    \n    counter = counter + 1\n    \n    \n","metadata":{"papermill":{"duration":1.479842,"end_time":"2024-03-01T22:39:43.322727","exception":false,"start_time":"2024-03-01T22:39:41.842885","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T10:02:28.140762Z","iopub.execute_input":"2024-03-05T10:02:28.141206Z","iopub.status.idle":"2024-03-05T10:02:56.101388Z","shell.execute_reply.started":"2024-03-05T10:02:28.141170Z","shell.execute_reply":"2024-03-05T10:02:56.100032Z"},"trusted":true},"execution_count":null,"outputs":[]}]}